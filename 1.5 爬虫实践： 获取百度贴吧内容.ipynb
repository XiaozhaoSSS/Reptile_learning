{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有的网页已经下载到本地！ 开始筛选信息。。。。\n",
      "出了点小问题\n",
      "出了点小问题\n",
      "当前页面爬取完成\n",
      "当前页面爬取完成\n",
      "出了点小问题\n",
      "出了点小问题\n",
      "出了点小问题\n",
      "当前页面爬取完成\n",
      "所有的信息都已经保存完毕！\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "抓取百度贴吧---生活大爆炸吧的基本内容\n",
    "爬虫线路： requests - bs4\n",
    "Python版本： 3.6\n",
    "OS： mac os 12.12.4\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 首先我们写好抓取网页的函数\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        # 这里我们知道百度贴吧的编码是utf-8，所以手动设置的。爬去其他的页面时建议使用：\n",
    "        # r.endcodding = r.apparent_endconding\n",
    "        r.encoding = 'utf-8'\n",
    "        return r.text\n",
    "    except:\n",
    "        return \" ERROR \"\n",
    "\n",
    "\n",
    "def get_content(url):\n",
    "    '''\n",
    "    分析贴吧的网页文件，整理信息，保存在列表变量中\n",
    "    '''\n",
    "\n",
    "    # 初始化一个列表来保存所有的帖子信息：\n",
    "    comments = []\n",
    "    # 首先，我们把需要爬取信息的网页下载到本地\n",
    "    html = get_html(url)\n",
    "\n",
    "    # 我们来做一锅汤\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # 按照之前的分析，我们找到所有具有‘ j_thread_list clearfix’属性的li标签。返回一个列表类型。\n",
    "    liTags = soup.find_all('li', attrs={'class': ' j_thread_list clearfix'})\n",
    "\n",
    "    # 通过循环找到每个帖子里的我们需要的信息：\n",
    "    for li in liTags:\n",
    "        # 初始化一个字典来存储文章信息\n",
    "        comment = {}\n",
    "        # 这里使用一个try except 防止爬虫找不到信息从而停止运行\n",
    "        try:\n",
    "            # 开始筛选信息，并保存到字典中\n",
    "            comment['title'] = li.find(\n",
    "                'a', attrs={'class': 'j_th_tit '}).text.strip()\n",
    "            comment['link'] = \"http://tieba.baidu.com/\" + \\\n",
    "                li.find('a', attrs={'class': 'j_th_tit '})['href']\n",
    "            comment['name'] = li.find(\n",
    "                'span', attrs={'class': 'tb_icon_author '}).text.strip()\n",
    "            comment['time'] = li.find(\n",
    "                'span', attrs={'class': 'pull-right is_show_create_time'}).text.strip()\n",
    "            comment['replyNum'] = li.find(\n",
    "                'span', attrs={'class': 'threadlist_rep_num center_text'}).text.strip()\n",
    "            comments.append(comment)\n",
    "        except:\n",
    "            print('出了点小问题')\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "def Out2File(dict):\n",
    "    '''\n",
    "    将爬取到的文件写入到本地\n",
    "    保存到当前目录的 TTBT.txt文件中。\n",
    "\n",
    "    '''\n",
    "    with open('TTBT.txt', 'a+', encoding='utf-8') as f:\n",
    "        for comment in dict:\n",
    "            f.write('标题： {} \\t 链接：{} \\t 发帖人：{} \\t 发帖时间：{} \\t 回复数量： {} \\n'.format(\n",
    "                comment['title'], comment['link'], comment['name'], comment['time'], comment['replyNum']))\n",
    "\n",
    "        print('当前页面爬取完成')\n",
    "\n",
    "\n",
    "def main(base_url, deep):\n",
    "    url_list = []\n",
    "    # 将所有需要爬去的url存入列表\n",
    "    for i in range(0, deep):\n",
    "        url_list.append(base_url + '&pn=' + str(50 * i))\n",
    "    print('所有的网页已经下载到本地！ 开始筛选信息。。。。')\n",
    "\n",
    "    #循环写入所有的数据\n",
    "    for url in url_list:\n",
    "        content = get_content(url)\n",
    "        Out2File(content)\n",
    "    print('所有的信息都已经保存完毕！')\n",
    "\n",
    "\n",
    "base_url = 'http://tieba.baidu.com/f?kw=%E7%94%9F%E6%B4%BB%E5%A4%A7%E7%88%86%E7%82%B8&ie=utf-8'\n",
    "# 设置需要爬取的页码数量\n",
    "deep = 3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(base_url, deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
